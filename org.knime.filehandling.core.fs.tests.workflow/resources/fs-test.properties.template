# Default configuration file for our file system integration tests.
#
# The purpose of this file (in source control) demonstrates  
# which settings each file system needs. Note that not every file
# system requires settings.
#
# ***IMPORTANT***
# DO NOT push any user names or secrets to git!!!
#
# Best practice is to copy this file to any location on your disk 
# and make an environment variable named 'KNIME_FS_TEST_PROPERTIES'
# point to it. This file can be populated with your credentials for
# testing.
#
# This file should only be updated when a new file system has been 
# implemented or an existing one has been modified. Then the file 
# system specific properties should be listed and checked in to 
# source control with dummy values.
#

# Limits the integration tests to only one file system. The currently available file systems are:
#   amazon-s3
#   dbfs
#   local
#   ftp
#   ftp-embedded
#   google-cs
#   google-drive
#   hdfs
#   hdfs-knox
#   hdfs-local-nio-wrapper
#   knime-local-relative-mountpoint
#   knime-local-relative-workflow
#   knime-relative-workflow-data
#   knime-rest
#   knime-rest-relative-mountpoint
#   knime-rest-relative-workflow
#   microsoft-blobstorage
#   microsoft-sharepoint
#   ssh
#   smb
# test-fs = local

################## Test fixtures for workflow tests ##################
# A local directory that contains test fixtures for workflow tests. If set, the "Create Test File
# System Connection" node will copy the fixtures to the file system under test, into a directory called "fixtures"
# below the working directory. Note that the fixtures being copied can be limited using the
# "Create Test File System Connection" node.
#
# fixture-dir = 


################ Amazon S3 ################ 
amazon-s3.workingDirPrefix = /knime-s3-file-handling-v2-test-temp
amazon-s3.region = eu-west-1
amazon-s3.accessKeyId = <YOUR-ACCESS_KEY-ID>
amazon-s3.accessKeySecret = <SECRET-FOR-YOUR-ACCESS-KEY>
amazon-s3.roleSwitchAccount = <YOUR-S3-SWITCH_ROLE-ACCOUNT>
amazon-s3.roleSwitchName = <ROLE-TO-SWITCH-TO>

################ Databricks DBFS ################
dbfs.workingDirPrefix = /knime-filehandling-test-dir
# dbfs typically looks like https://<some-id>.cloud.databricks.com/
dbfs.url = <DATABRICKS-DEPLOYMENT-URL>
dbfs.token = <TOKEN>

################ FTP ###################
ftp.workingDirPrefix = <PATH-ON-FTP-SERVER>
ftp.host = <HOST>
# port is optional
# ftp.port = 21
# ftps (TLS-encrypted FTP) is optional
# ftp.ftps = false
# timeZoneOffset (in minutes) is optional
# ftp.timeZoneOffset = 0
ftp.username = <USERNAME>
ftp.password = <PASSWORD>

################ FTP (with embedded server) ###################
ftp-embedded.workingDirPrefix = <PATH-ON-LOCAL-SYSTEM>
ftp-embedded.username = <USERNAME>
ftp-embedded.password = <PASSWORD>

################ Google CS (Cloud Storage) ################
google-cs.workingDirPrefix = /knime-filehandling-test-bucket
google-cs.email = <YOUR-SERVICE-ACCOUNT-EMAIL>
google-cs.keyFilePath = <P12-KEY-FILE-LOCATION>
google-cs.projectId = <PROJECT-ID>

################ Google Drive ################
google-drive.workingDirPrefix = /My Drive
google-drive.email = <YOUR-SERVICE-ACCOUNT-EMAIL>
google-drive.keyFilePath = <P12-KEY-FILE-LOCATION>

################ HDFS ################
# Example url: webhdfs://test-cluster:50070
hdfs.workingDirPrefix = /tmp/
# <PROTOCOL> must be one of HDFS, WEBHDFS, WEBHDFS_SSL, HTTPFS, HTTPFS_SSL
hdfs.protocol = HTTPFS
hdfs.host = <HOST>
# hdfs.port = <PORT> (optional)
# authentication type: simple or kerberos
hdfs.auth = simple
hdfs.user = <YOUR-USERNAME>

################ HDFS via KNOX ################
hdfs-knox.url = https://<HOST>:<PORT>/gateway/<TOPOLOGY>
hdfs-knox.workingDirPrefix = /tmp/file-handling-tests
hdfs-knox.user = <YOUR-USERNAME>
hdfs-knox.pass = <YOUR-PASSWORD>

################ KNIME: REST file system (connects to Server) ################
knime-rest.workingDirPrefix = <PATH-IN-SERVER-REPO>
# Example url: http://localhost:8080/knime/rest/v4/repository
knime-rest.url = <REST-URL>
knime-rest.user = <USERNAME>
knime-rest.pass = <PASSWORD>

################ KNIME: Server-side mountpoint-relative file system (connects to Server)  #######
knime-rest-relative-mountpoint.workingDirPrefix = <PATH-IN-SERVER-REPO>
# Example url: http://localhost:8080/knime/rest/v4/repository
knime-rest-relative-mountpoint.url = <REST-URL>
knime-rest-relative-mountpoint.user = <USERNAME>
knime-rest-relative-mountpoint.pass = <PASSWORD>

################ KNIME: Server-side workflow-relative file system (connects to Server)  ########
knime-rest-relative-workflow.workingDirPrefix = <PATH-IN-SERVER-REPO>
# Example url: http://localhost:8080/knime/rest/v4/repository
knime-rest-relative-workflow.url = <REST-URL>
knime-rest-relative-workflow.user = <USERNAME>
knime-rest-relative-workflow.pass = <PASSWORD>

################ Microsoft Azure Blob Storage  ################
microsoft-blobstorage.workingDirPrefix = /knime-filehandling-test-container
microsoft-blobstorage.account = <STORAGE-ACCOUNT>
microsoft-blobstorage.key = <STORAGE-ACCOUNT-KEY>

################ Microsoft SharePoint Online ################
microsoft-sharepoint.workingDirPrefix = /knime-filehandling-test-drive
microsoft-sharepoint.siteWebURL = https://knimedev.sharepoint.com/
microsoft-sharepoint.username = <USERNAME>
microsoft-sharepoint.password = <PASSWORD>
microsoft-sharepoint.tenant = <TENANT-ID>

################ SSH ###################
ssh.workingDirPrefix = <PATH-ON-SSH-SERVER>
ssh.host = <HOST>
ssh.username = <USERNAME>
ssh.password = <PASSWORD>
# password and keyFile authentication are mutually exclusive
# ssh.keyFile = <PATH-TO-PRIVATE-KEY>

################ SMB #######################
smb.workingDirPrefix = \\knime-filehandling-test
smb.host = <HOST>
smb.share = <SHARE-NAME>
smb.username = <USERNAME>
smb.password = <PASSWORD>
